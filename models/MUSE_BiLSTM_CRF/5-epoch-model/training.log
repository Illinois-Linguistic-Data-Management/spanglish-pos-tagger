2023-02-28 01:00:06,371 ----------------------------------------------------------------------------------------------------
2023-02-28 01:00:06,373 Model: "SequenceTagger(
  (embeddings): SpanglishBilingualEmbeddings()
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (rnn): LSTM(300, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=21, bias=True)
  (loss_function): ViterbiLoss()
  (crf): CRF()
)"
2023-02-28 01:00:06,375 ----------------------------------------------------------------------------------------------------
2023-02-28 01:00:06,376 Corpus: "MultiCorpus: 32268 train + 4016 dev + 3187 test sentences
 - UD_ENGLISH Corpus: 12544 train + 2001 dev + 2077 test sentences - /root/.flair/datasets/ud_english
 - UD_SPANISH Corpus: 14187 train + 1400 dev + 426 test sentences - /root/.flair/datasets/ud_spanish
 - ColumnCorpus Corpus: 5537 train + 615 dev + 684 test sentences - ./"
2023-02-28 01:00:06,377 ----------------------------------------------------------------------------------------------------
2023-02-28 01:00:06,379 Parameters:
2023-02-28 01:00:06,380  - learning_rate: "0.100000"
2023-02-28 01:00:06,382  - mini_batch_size: "32"
2023-02-28 01:00:06,383  - patience: "3"
2023-02-28 01:00:06,384  - anneal_factor: "0.5"
2023-02-28 01:00:06,386  - max_epochs: "50"
2023-02-28 01:00:06,387  - shuffle: "True"
2023-02-28 01:00:06,388  - train_with_dev: "True"
2023-02-28 01:00:06,390  - batch_growth_annealing: "False"
2023-02-28 01:00:06,391 ----------------------------------------------------------------------------------------------------
2023-02-28 01:00:06,391 Model training base path: "upos-spanglish-MUSE-BiLSTM-CRF"
2023-02-28 01:00:06,392 ----------------------------------------------------------------------------------------------------
2023-02-28 01:00:06,395 Device: cuda:0
2023-02-28 01:00:06,396 ----------------------------------------------------------------------------------------------------
2023-02-28 01:00:06,397 Embeddings storage mode: cpu
2023-02-28 01:00:06,399 ----------------------------------------------------------------------------------------------------
2023-02-28 01:00:31,173 epoch 1 - iter 113/1134 - loss 2.30369053 - samples/sec: 244.61 - lr: 0.100000
2023-02-28 01:00:52,877 epoch 1 - iter 226/1134 - loss 1.83929670 - samples/sec: 243.25 - lr: 0.100000
2023-02-28 01:01:13,766 epoch 1 - iter 339/1134 - loss 1.57325004 - samples/sec: 234.20 - lr: 0.100000
2023-02-28 01:01:44,683 epoch 1 - iter 452/1134 - loss 1.35787386 - samples/sec: 169.79 - lr: 0.100000
2023-02-28 01:02:20,129 epoch 1 - iter 565/1134 - loss 1.17214223 - samples/sec: 144.35 - lr: 0.100000
2023-02-28 01:02:56,671 epoch 1 - iter 678/1134 - loss 1.03968849 - samples/sec: 139.91 - lr: 0.100000
2023-02-28 01:03:33,723 epoch 1 - iter 791/1134 - loss 0.94583308 - samples/sec: 138.39 - lr: 0.100000
2023-02-28 01:03:56,676 epoch 1 - iter 904/1134 - loss 0.92486955 - samples/sec: 184.16 - lr: 0.100000
2023-02-28 01:04:12,347 epoch 1 - iter 1017/1134 - loss 0.92037337 - samples/sec: 234.17 - lr: 0.100000
2023-02-28 01:04:36,506 epoch 1 - iter 1130/1134 - loss 0.89844773 - samples/sec: 197.42 - lr: 0.100000
2023-02-28 01:04:37,001 ----------------------------------------------------------------------------------------------------
2023-02-28 01:04:37,003 EPOCH 1 done: loss 0.8982 - lr 0.100000
2023-02-28 01:04:37,004 BAD EPOCHS (no improvement): 0
2023-02-28 01:04:37,006 ----------------------------------------------------------------------------------------------------
2023-02-28 01:05:05,298 epoch 2 - iter 113/1134 - loss 0.68234913 - samples/sec: 180.59 - lr: 0.100000
2023-02-28 01:05:33,918 epoch 2 - iter 226/1134 - loss 0.66827176 - samples/sec: 177.45 - lr: 0.100000
2023-02-28 01:06:02,581 epoch 2 - iter 339/1134 - loss 0.66154936 - samples/sec: 178.31 - lr: 0.100000
2023-02-28 01:06:30,238 epoch 2 - iter 452/1134 - loss 0.65732320 - samples/sec: 186.17 - lr: 0.100000
2023-02-28 01:06:57,294 epoch 2 - iter 565/1134 - loss 0.65224443 - samples/sec: 175.76 - lr: 0.100000
2023-02-28 01:07:26,834 epoch 2 - iter 678/1134 - loss 0.64476484 - samples/sec: 186.10 - lr: 0.100000
2023-02-28 01:07:52,391 epoch 2 - iter 791/1134 - loss 0.64078226 - samples/sec: 188.50 - lr: 0.100000
2023-02-28 01:08:20,257 epoch 2 - iter 904/1134 - loss 0.63775198 - samples/sec: 169.03 - lr: 0.100000
2023-02-28 01:08:48,336 epoch 2 - iter 1017/1134 - loss 0.63475002 - samples/sec: 183.07 - lr: 0.100000
2023-02-28 01:09:16,537 epoch 2 - iter 1130/1134 - loss 0.63329928 - samples/sec: 180.71 - lr: 0.100000
2023-02-28 01:09:17,520 ----------------------------------------------------------------------------------------------------
2023-02-28 01:09:17,522 EPOCH 2 done: loss 0.6333 - lr 0.100000
2023-02-28 01:09:17,523 BAD EPOCHS (no improvement): 0
2023-02-28 01:09:17,526 ----------------------------------------------------------------------------------------------------
2023-02-28 01:09:46,646 epoch 3 - iter 113/1134 - loss 0.61694115 - samples/sec: 174.42 - lr: 0.100000
2023-02-28 01:10:15,259 epoch 3 - iter 226/1134 - loss 0.60562006 - samples/sec: 178.23 - lr: 0.100000
2023-02-28 01:10:43,476 epoch 3 - iter 339/1134 - loss 0.60325659 - samples/sec: 166.40 - lr: 0.100000
2023-02-28 01:11:11,243 epoch 3 - iter 452/1134 - loss 0.60092790 - samples/sec: 185.88 - lr: 0.100000
2023-02-28 01:11:40,199 epoch 3 - iter 565/1134 - loss 0.59985218 - samples/sec: 175.97 - lr: 0.100000
2023-02-28 01:12:09,443 epoch 3 - iter 678/1134 - loss 0.59682970 - samples/sec: 174.72 - lr: 0.100000
2023-02-28 01:12:37,820 epoch 3 - iter 791/1134 - loss 0.59539578 - samples/sec: 180.81 - lr: 0.100000
2023-02-28 01:13:06,545 epoch 3 - iter 904/1134 - loss 0.59521327 - samples/sec: 177.48 - lr: 0.100000
2023-02-28 01:13:34,992 epoch 3 - iter 1017/1134 - loss 0.59455106 - samples/sec: 179.89 - lr: 0.100000
2023-02-28 01:14:02,923 epoch 3 - iter 1130/1134 - loss 0.59454950 - samples/sec: 184.32 - lr: 0.100000
2023-02-28 01:14:03,798 ----------------------------------------------------------------------------------------------------
2023-02-28 01:14:03,801 EPOCH 3 done: loss 0.5945 - lr 0.100000
2023-02-28 01:14:03,803 BAD EPOCHS (no improvement): 0
2023-02-28 01:14:03,805 ----------------------------------------------------------------------------------------------------
2023-02-28 01:14:32,191 epoch 4 - iter 113/1134 - loss 0.58108784 - samples/sec: 180.52 - lr: 0.100000
2023-02-28 01:15:00,319 epoch 4 - iter 226/1134 - loss 0.57905348 - samples/sec: 182.37 - lr: 0.100000
2023-02-28 01:15:28,886 epoch 4 - iter 339/1134 - loss 0.57677541 - samples/sec: 180.16 - lr: 0.100000
2023-02-28 01:15:58,008 epoch 4 - iter 452/1134 - loss 0.57665398 - samples/sec: 176.79 - lr: 0.100000
2023-02-28 01:16:27,625 epoch 4 - iter 565/1134 - loss 0.57620280 - samples/sec: 171.68 - lr: 0.100000
2023-02-28 01:16:56,468 epoch 4 - iter 678/1134 - loss 0.57553960 - samples/sec: 177.87 - lr: 0.100000
2023-02-28 01:17:25,581 epoch 4 - iter 791/1134 - loss 0.57634850 - samples/sec: 175.65 - lr: 0.100000
2023-02-28 01:17:53,784 epoch 4 - iter 904/1134 - loss 0.57572908 - samples/sec: 183.46 - lr: 0.100000
2023-02-28 01:18:23,008 epoch 4 - iter 1017/1134 - loss 0.57560976 - samples/sec: 174.88 - lr: 0.100000
2023-02-28 01:18:51,709 epoch 4 - iter 1130/1134 - loss 0.57605011 - samples/sec: 178.40 - lr: 0.100000
2023-02-28 01:18:52,517 ----------------------------------------------------------------------------------------------------
2023-02-28 01:18:52,519 EPOCH 4 done: loss 0.5763 - lr 0.100000
2023-02-28 01:18:52,521 BAD EPOCHS (no improvement): 0
2023-02-28 01:18:52,523 ----------------------------------------------------------------------------------------------------
2023-02-28 01:19:21,089 epoch 5 - iter 113/1134 - loss 0.55803378 - samples/sec: 181.12 - lr: 0.100000
2023-02-28 01:19:49,757 epoch 5 - iter 226/1134 - loss 0.55766004 - samples/sec: 179.45 - lr: 0.100000
2023-02-28 01:20:19,170 epoch 5 - iter 339/1134 - loss 0.56364291 - samples/sec: 173.23 - lr: 0.100000
2023-02-28 01:20:47,705 epoch 5 - iter 452/1134 - loss 0.56086359 - samples/sec: 180.71 - lr: 0.100000
2023-02-28 01:21:14,741 epoch 5 - iter 565/1134 - loss 0.56195026 - samples/sec: 176.24 - lr: 0.100000
2023-02-28 01:21:43,308 epoch 5 - iter 678/1134 - loss 0.56119940 - samples/sec: 180.99 - lr: 0.100000
2023-02-28 01:22:13,281 epoch 5 - iter 791/1134 - loss 0.56245002 - samples/sec: 168.93 - lr: 0.100000
2023-02-28 01:22:41,412 epoch 5 - iter 904/1134 - loss 0.56203594 - samples/sec: 183.87 - lr: 0.100000
2023-02-28 01:23:10,599 epoch 5 - iter 1017/1134 - loss 0.56296677 - samples/sec: 175.15 - lr: 0.100000
2023-02-28 01:23:39,841 epoch 5 - iter 1130/1134 - loss 0.56343891 - samples/sec: 174.13 - lr: 0.100000
2023-02-28 01:23:40,668 ----------------------------------------------------------------------------------------------------
2023-02-28 01:23:40,671 EPOCH 5 done: loss 0.5634 - lr 0.100000
2023-02-28 01:23:40,673 BAD EPOCHS (no improvement): 0
2023-02-28 01:23:40,675 ----------------------------------------------------------------------------------------------------
2023-02-28 01:23:41,565 ----------------------------------------------------------------------------------------------------
2023-02-28 01:23:41,567 Exiting from training early.
2023-02-28 01:23:41,568 Saving model ...
2023-02-28 01:23:55,323 Done.
2023-02-28 01:23:55,933 ----------------------------------------------------------------------------------------------------
2023-02-28 01:23:55,935 Testing using last state of model ...
2023-02-28 01:24:17,880 Evaluating as a multi-label problem: False
2023-02-28 01:24:18,269 0.8576	0.8576	0.8576	0.8576
2023-02-28 01:24:18,271 
Results:
- F-score (micro) 0.8576
- F-score (macro) 0.7313
- Accuracy 0.8576

By class:
              precision    recall  f1-score   support

        NOUN     0.8335    0.8463    0.8398      7247
       PUNCT     0.9545    0.9769    0.9655      5707
        VERB     0.8801    0.8427    0.8610      5460
         ADP     0.9217    0.9354    0.9285      4553
         DET     0.9094    0.9357    0.9224      4247
        PRON     0.8615    0.9214    0.8905      3842
       PROPN     0.6754    0.7612    0.7158      2802
         ADJ     0.7730    0.6930    0.7308      3007
         ADV     0.7840    0.8226    0.8029      2458
         AUX     0.8298    0.9172    0.8713      1860
       CCONJ     0.9285    0.8793    0.9032      1964
         NUM     0.9041    0.8103    0.8546       838
        PART     0.8041    0.8561    0.8293       681
       SCONJ     0.7672    0.6181    0.6846       720
        INTJ     0.7913    0.5630    0.6579       357
           X     0.3030    0.0662    0.1087       453
         SYM     0.8696    0.4545    0.5970       132
        PREP     0.0000    0.0000    0.0000        26

    accuracy                         0.8576     46354
   macro avg     0.7662    0.7167    0.7313     46354
weighted avg     0.8535    0.8576    0.8537     46354

2023-02-28 01:24:18,272 ----------------------------------------------------------------------------------------------------
2023-02-28 01:24:18,275 ----------------------------------------------------------------------------------------------------
2023-02-28 01:24:30,197 Evaluating as a multi-label problem: False
2023-02-28 01:24:30,400 /root/.flair/datasets/ud_english
2023-02-28 01:24:30,402 0.8548	0.8548	0.8548	0.8548
2023-02-28 01:24:30,403 ----------------------------------------------------------------------------------------------------
2023-02-28 01:24:36,695 Evaluating as a multi-label problem: False
2023-02-28 01:24:36,792 /root/.flair/datasets/ud_spanish
2023-02-28 01:24:36,794 0.9083	0.9083	0.9083	0.9082
2023-02-28 01:24:36,796 ----------------------------------------------------------------------------------------------------
2023-02-28 01:24:40,733 Evaluating as a multi-label problem: False
2023-02-28 01:24:40,806 ./
2023-02-28 01:24:40,809 0.7994	0.7994	0.7994	0.7994
les/sec: 176.79 - lr: 0.100000
2023-02-28 01:16:27,625 epoch 4 - iter 565/1134 - loss 0.57620280 - samples/sec: 171.68 - lr: 0.100000
2023-02-28 01:16:56,468 epoch 4 - iter 678/1134 - loss 0.57553960 - samples/sec: 177.87 - lr: 0.100000
2023-02-28 01:17:25,581 epoch 4 - iter 791/1134 - loss 0.57634850 - samples/sec: 175.65 - lr: 0.100000
2023-02-28 01:17:53,784 epoch 4 - iter 904/1134 - loss 0.57572908 - samples/sec: 183.46 - lr: 0.100000
2023-02-28 01:18:23,008 epoch 4 - iter 1017/1134 - loss 0.57560976 - samples/sec: 174.88 - lr: 0.100000
2023-02-28 01:18:51,709 epoch 4 - iter 1130/1134 - loss 0.57605011 - samples/sec: 178.40 - lr: 0.100000
2023-02-28 01:18:52,517 ----------------------------------------------------------------------------------------------------
2023-02-28 01:18:52,519 EPOCH 4 done: loss 0.5763 - lr 0.100000
2023-02-28 01:18:52,521 BAD EPOCHS (no improvement): 0
2023-02-28 01:18:52,523 ----------------------------------------------------------------------------------------------------
2023-02-28 01:19:21,089 epoch 5 - iter 113/1134 - loss 0.55803378 - samples/sec: 181.12 - lr: 0.100000
2023-02-28 01:19:49,757 epoch 5 - iter 226/1134 - loss 0.55766004 - samples/sec: 179.45 - lr: 0.100000
2023-02-28 01:20:19,170 epoch 5 - iter 339/1134 - loss 0.56364291 - samples/sec: 173.23 - lr: 0.100000
2023-02-28 01:20:47,705 epoch 5 - iter 452/1134 - loss 0.56086359 - samples/sec: 180.71 - lr: 0.100000
2023-02-28 01:21:14,741 epoch 5 - iter 565/1134 - loss 0.56195026 - samples/sec: 176.24 - lr: 0.100000
2023-02-28 01:21:43,308 epoch 5 - iter 678/1134 - loss 0.56119940 - samples/sec: 180.99 - lr: 0.100000
2023-02-28 01:22:13,281 epoch 5 - iter 791/1134 - loss 0.56245002 - samples/sec: 168.93 - lr: 0.100000
2023-02-28 01:22:41,412 epoch 5 - iter 904/1134 - loss 0.56203594 - samples/sec: 183.87 - lr: 0.100000
2023-02-28 01:23:10,599 epoch 5 - iter 1017/1134 - loss 0.56296677 - samples/sec: 175.15 - lr: 0.100000
2023-02-28 01:23:39,841 epoch 5 - iter 1130/1134 - loss 0.56343891 - samples/sec: 174.13 - lr: 0.100000
2023-02-28 01:23:40,668 ----------------------------------------------------------------------------------------------------
2023-02-28 01:23:40,671 EPOCH 5 done: loss 0.5634 - lr 0.100000
2023-02-28 01:23:40,673 BAD EPOCHS (no improvement): 0
2023-02-28 01:23:40,675 ----------------------------------------------------------------------------------------------------
2023-02-28 01:23:41,565 ----------------------------------------------------------------------------------------------------
2023-02-28 01:23:41,567 Exiting from training early.
2023-02-28 01:23:41,568 Saving model ...
2023-02-28 01:23:55,323 Done.
2023-02-28 01:23:55,933 ----------------------------------------------------------------------------------------------------
2023-02-28 01:23:55,935 Testing using last state of model ...
2023-02-28 01:24:17,880 Evaluating as a multi-label problem: False
2023-02-28 01:24:18,269 0.8576	0.8576	0.8576	0.8576
2023-02-28 01:24:18,271 
Results:
- F-score (micro) 0.8576
- F-score (macro) 0.7313
- Accuracy 0.8576

By class:
              precision    recall  f1-score   support

        NOUN     0.8335    0.8463    0.8398      7247
       PUNCT     0.9545    0.9769    0.9655      5707
        VERB     0.8801    0.8427    0.8610      5460
         ADP     0.9217    0.9354    0.9285      4553
         DET     0.9094    0.9357    0.9224      4247
        PRON     0.8615    0.9214    0.8905      3842
       PROPN     0.6754    0.7612    0.7158      2802
         ADJ     0.7730    0.6930    0.7308      3007
         ADV     0.7840    0.8226    0.8029      2458
         AUX     0.8298    0.9172    0.8713      1860
       CCONJ     0.9285    0.8793    0.9032      1964
         NUM     0.9041    0.8103    0.8546       838
        PART     0.8041    0.8561    0.8293       681
       SCONJ     0.7672    0.6181    0.6846       720
        INTJ     0.7913    0.5630    0.6579       357
           X     0.3030    0.0662    0.1087       453
         SYM     0.8696    0.4545    0.5970       132
        PREP     0.0000    0.0000    0.0000        26

    accuracy                         0.8576     46354
   macro avg     0.7662    0.7167    0.7313     46354
weighted avg     0.8535    0.8576    0.8537     46354

2023-02-28 01:24:18,272 ----------------------------------------------------------------------------------------------------
2023-02-28 01:24:18,275 ----------------------------------------------------------------------------------------------------
2023-02-28 01:24:30,197 Evaluating as a multi-label problem: False
2023-02-28 01:24:30,400 /root/.flair/datasets/ud_english
2023-02-28 01:24:30,402 0.8548	0.8548	0.8548	0.8548
2023-02-28 01:24:30,403 ----------------------------------------------------------------------------------------------------
2023-02-28 01:24:36,695 Evaluating as a multi-label problem: False
2023-02-28 01:24:36,792 /root/.flair/datasets/ud_spanish
2023-02-28 01:24:36,794 0.9083	0.9083	0.9083	0.9082
2023-02-28 01:24:36,796 ----------------------------------------------------------------------------------------------------
2023-02-28 01:24:40,733 Evaluating as a multi-label problem: False
2023-02-28 01:24:40,806 ./
2023-02-28 01:24:40,809 0.7994	0.7994	0.7994	0.7994
