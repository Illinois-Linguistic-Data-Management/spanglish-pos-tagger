2023-02-02 23:49:29,083 ----------------------------------------------------------------------------------------------------
2023-02-02 23:49:29,088 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (list_embedding_1): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(31002, 768, padding_idx=1)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (rnn): LSTM(1536, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=18, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-02-02 23:49:29,092 ----------------------------------------------------------------------------------------------------
2023-02-02 23:49:29,093 Corpus: "MultiCorpus: 26731 train + 3401 dev + 2503 test sentences
 - UD_ENGLISH Corpus: 12544 train + 2001 dev + 2077 test sentences - /root/.flair/datasets/ud_english
 - UD_SPANISH Corpus: 14187 train + 1400 dev + 426 test sentences - /root/.flair/datasets/ud_spanish"
2023-02-02 23:49:29,095 ----------------------------------------------------------------------------------------------------
2023-02-02 23:49:29,099 Parameters:
2023-02-02 23:49:29,100  - learning_rate: "0.100000"
2023-02-02 23:49:29,103  - mini_batch_size: "32"
2023-02-02 23:49:29,104  - patience: "3"
2023-02-02 23:49:29,106  - anneal_factor: "0.5"
2023-02-02 23:49:29,109  - max_epochs: "10"
2023-02-02 23:49:29,110  - shuffle: "True"
2023-02-02 23:49:29,112  - train_with_dev: "True"
2023-02-02 23:49:29,113  - batch_growth_annealing: "False"
2023-02-02 23:49:29,118 ----------------------------------------------------------------------------------------------------
2023-02-02 23:49:29,120 Model training base path: "upos-spanglish-BERT-BETO-top-last"
2023-02-02 23:49:29,122 ----------------------------------------------------------------------------------------------------
2023-02-02 23:49:29,123 Device: cuda:0
2023-02-02 23:49:29,127 ----------------------------------------------------------------------------------------------------
2023-02-02 23:49:29,129 Embeddings storage mode: cpu
2023-02-02 23:49:29,130 ----------------------------------------------------------------------------------------------------
2023-02-02 23:50:18,887 epoch 1 - iter 94/942 - loss 1.27905339 - samples/sec: 66.23 - lr: 0.100000
2023-02-02 23:51:01,186 epoch 1 - iter 188/942 - loss 0.89563682 - samples/sec: 77.60 - lr: 0.100000
2023-02-02 23:51:51,723 epoch 1 - iter 282/942 - loss 0.68604187 - samples/sec: 64.90 - lr: 0.100000
2023-02-02 23:52:37,673 epoch 1 - iter 376/942 - loss 0.58417487 - samples/sec: 70.99 - lr: 0.100000
2023-02-02 23:53:48,912 epoch 1 - iter 470/942 - loss 0.52159218 - samples/sec: 46.97 - lr: 0.100000
2023-02-02 23:54:59,834 epoch 1 - iter 564/942 - loss 0.45812667 - samples/sec: 46.98 - lr: 0.100000
2023-02-02 23:56:13,055 epoch 1 - iter 658/942 - loss 0.41350378 - samples/sec: 45.61 - lr: 0.100000
2023-02-02 23:57:29,280 epoch 1 - iter 752/942 - loss 0.37990299 - samples/sec: 43.31 - lr: 0.100000
2023-02-02 23:58:39,531 epoch 1 - iter 846/942 - loss 0.35670235 - samples/sec: 47.17 - lr: 0.100000
2023-02-02 23:59:31,135 epoch 1 - iter 940/942 - loss 0.34616164 - samples/sec: 64.07 - lr: 0.100000
2023-02-02 23:59:32,223 ----------------------------------------------------------------------------------------------------
2023-02-02 23:59:32,225 EPOCH 1 done: loss 0.3458 - lr 0.100000
2023-02-02 23:59:32,227 BAD EPOCHS (no improvement): 0
2023-02-02 23:59:32,230 ----------------------------------------------------------------------------------------------------
2023-02-03 00:00:31,584 epoch 2 - iter 94/942 - loss 0.19524344 - samples/sec: 54.85 - lr: 0.100000
2023-02-03 00:01:33,822 epoch 2 - iter 188/942 - loss 0.18759872 - samples/sec: 53.22 - lr: 0.100000
2023-02-03 00:02:36,974 epoch 2 - iter 282/942 - loss 0.18797907 - samples/sec: 52.46 - lr: 0.100000
2023-02-03 00:03:38,903 epoch 2 - iter 376/942 - loss 0.18609887 - samples/sec: 53.73 - lr: 0.100000
2023-02-03 00:04:42,303 epoch 2 - iter 470/942 - loss 0.18424220 - samples/sec: 52.26 - lr: 0.100000
2023-02-03 00:05:46,543 epoch 2 - iter 564/942 - loss 0.18205910 - samples/sec: 51.76 - lr: 0.100000
2023-02-03 00:06:51,182 epoch 2 - iter 658/942 - loss 0.17978378 - samples/sec: 51.32 - lr: 0.100000
2023-02-03 00:07:55,271 epoch 2 - iter 752/942 - loss 0.17727161 - samples/sec: 51.36 - lr: 0.100000
2023-02-03 00:08:58,555 epoch 2 - iter 846/942 - loss 0.17602003 - samples/sec: 52.04 - lr: 0.100000
2023-02-03 00:10:02,517 epoch 2 - iter 940/942 - loss 0.17419536 - samples/sec: 51.83 - lr: 0.100000
2023-02-03 00:10:03,511 ----------------------------------------------------------------------------------------------------
2023-02-03 00:10:03,513 EPOCH 2 done: loss 0.1741 - lr 0.100000
2023-02-03 00:10:03,514 BAD EPOCHS (no improvement): 0
2023-02-03 00:10:03,516 ----------------------------------------------------------------------------------------------------
2023-02-03 00:11:05,827 epoch 3 - iter 94/942 - loss 0.13866429 - samples/sec: 52.86 - lr: 0.100000
2023-02-03 00:12:08,897 epoch 3 - iter 188/942 - loss 0.13618984 - samples/sec: 52.38 - lr: 0.100000
2023-02-03 00:13:12,920 epoch 3 - iter 282/942 - loss 0.13751791 - samples/sec: 51.77 - lr: 0.100000
2023-02-03 00:14:15,631 epoch 3 - iter 376/942 - loss 0.13665155 - samples/sec: 52.72 - lr: 0.100000
2023-02-03 00:15:18,581 epoch 3 - iter 470/942 - loss 0.13574160 - samples/sec: 52.13 - lr: 0.100000
2023-02-03 00:16:22,376 epoch 3 - iter 564/942 - loss 0.13506509 - samples/sec: 51.21 - lr: 0.100000
2023-02-03 00:17:25,090 epoch 3 - iter 658/942 - loss 0.13353872 - samples/sec: 52.73 - lr: 0.100000
2023-02-03 00:18:29,312 epoch 3 - iter 752/942 - loss 0.13299045 - samples/sec: 51.64 - lr: 0.100000
2023-02-03 00:19:32,290 epoch 3 - iter 846/942 - loss 0.13246578 - samples/sec: 52.48 - lr: 0.100000
2023-02-03 00:20:36,199 epoch 3 - iter 940/942 - loss 0.13188473 - samples/sec: 51.66 - lr: 0.100000
2023-02-03 00:20:37,253 ----------------------------------------------------------------------------------------------------
2023-02-03 00:20:37,255 EPOCH 3 done: loss 0.1319 - lr 0.100000
2023-02-03 00:20:37,257 BAD EPOCHS (no improvement): 0
2023-02-03 00:20:37,258 ----------------------------------------------------------------------------------------------------
2023-02-03 00:21:41,132 epoch 4 - iter 94/942 - loss 0.10958540 - samples/sec: 51.28 - lr: 0.100000
2023-02-03 00:22:45,729 epoch 4 - iter 188/942 - loss 0.10807377 - samples/sec: 51.30 - lr: 0.100000
2023-02-03 00:23:49,749 epoch 4 - iter 282/942 - loss 0.10755318 - samples/sec: 51.17 - lr: 0.100000
2023-02-03 00:24:54,100 epoch 4 - iter 376/942 - loss 0.10695862 - samples/sec: 51.33 - lr: 0.100000
2023-02-03 00:25:56,160 epoch 4 - iter 470/942 - loss 0.10664151 - samples/sec: 53.16 - lr: 0.100000
2023-02-03 00:26:59,351 epoch 4 - iter 564/942 - loss 0.10600493 - samples/sec: 52.47 - lr: 0.100000
2023-02-03 00:28:02,313 epoch 4 - iter 658/942 - loss 0.10603873 - samples/sec: 52.72 - lr: 0.100000
2023-02-03 00:29:04,876 epoch 4 - iter 752/942 - loss 0.10577091 - samples/sec: 52.62 - lr: 0.100000
2023-02-03 00:30:07,475 epoch 4 - iter 846/942 - loss 0.10598063 - samples/sec: 53.25 - lr: 0.100000
2023-02-03 00:31:08,110 epoch 4 - iter 940/942 - loss 0.10524094 - samples/sec: 54.59 - lr: 0.100000
2023-02-03 00:31:09,076 ----------------------------------------------------------------------------------------------------
2023-02-03 00:31:09,078 EPOCH 4 done: loss 0.1052 - lr 0.100000
2023-02-03 00:31:09,080 BAD EPOCHS (no improvement): 0
2023-02-03 00:31:09,082 ----------------------------------------------------------------------------------------------------
2023-02-03 00:32:12,646 epoch 5 - iter 94/942 - loss 0.08827221 - samples/sec: 52.19 - lr: 0.100000
2023-02-03 00:33:17,535 epoch 5 - iter 188/942 - loss 0.08927328 - samples/sec: 50.89 - lr: 0.100000
2023-02-03 00:34:22,169 epoch 5 - iter 282/942 - loss 0.08843147 - samples/sec: 50.88 - lr: 0.100000
2023-02-03 00:35:25,573 epoch 5 - iter 376/942 - loss 0.08737134 - samples/sec: 52.48 - lr: 0.100000
2023-02-03 00:36:29,848 epoch 5 - iter 470/942 - loss 0.08685366 - samples/sec: 51.59 - lr: 0.100000
2023-02-03 00:37:33,000 epoch 5 - iter 564/942 - loss 0.08759941 - samples/sec: 52.15 - lr: 0.100000
2023-02-03 00:38:34,958 epoch 5 - iter 658/942 - loss 0.08785349 - samples/sec: 53.62 - lr: 0.100000
2023-02-03 00:39:38,701 epoch 5 - iter 752/942 - loss 0.08781822 - samples/sec: 52.05 - lr: 0.100000
2023-02-03 00:40:41,115 epoch 5 - iter 846/942 - loss 0.08782056 - samples/sec: 52.57 - lr: 0.100000
2023-02-03 00:41:44,244 epoch 5 - iter 940/942 - loss 0.08811778 - samples/sec: 51.72 - lr: 0.100000
2023-02-03 00:41:45,453 ----------------------------------------------------------------------------------------------------
2023-02-03 00:41:45,455 EPOCH 5 done: loss 0.0881 - lr 0.100000
2023-02-03 00:41:45,457 BAD EPOCHS (no improvement): 0
2023-02-03 00:41:45,458 ----------------------------------------------------------------------------------------------------
2023-02-03 00:42:49,675 epoch 6 - iter 94/942 - loss 0.07296360 - samples/sec: 51.41 - lr: 0.100000
2023-02-03 00:43:53,179 epoch 6 - iter 188/942 - loss 0.07399933 - samples/sec: 51.81 - lr: 0.100000
2023-02-03 00:44:57,664 epoch 6 - iter 282/942 - loss 0.07390917 - samples/sec: 51.30 - lr: 0.100000
2023-02-03 00:46:00,961 epoch 6 - iter 376/942 - loss 0.07437637 - samples/sec: 51.79 - lr: 0.100000
2023-02-03 00:47:03,723 epoch 6 - iter 470/942 - loss 0.07477545 - samples/sec: 52.25 - lr: 0.100000
2023-02-03 00:48:06,861 epoch 6 - iter 564/942 - loss 0.07414798 - samples/sec: 52.51 - lr: 0.100000
2023-02-03 00:49:09,356 epoch 6 - iter 658/942 - loss 0.07473933 - samples/sec: 52.90 - lr: 0.100000
2023-02-03 00:50:11,525 epoch 6 - iter 752/942 - loss 0.07488605 - samples/sec: 53.18 - lr: 0.100000
2023-02-03 00:51:13,770 epoch 6 - iter 846/942 - loss 0.07497969 - samples/sec: 53.11 - lr: 0.100000
2023-02-03 00:52:15,841 epoch 6 - iter 940/942 - loss 0.07511034 - samples/sec: 53.45 - lr: 0.100000
2023-02-03 00:52:16,870 ----------------------------------------------------------------------------------------------------
2023-02-03 00:52:16,871 EPOCH 6 done: loss 0.0751 - lr 0.100000
2023-02-03 00:52:16,873 BAD EPOCHS (no improvement): 0
2023-02-03 00:52:16,875 ----------------------------------------------------------------------------------------------------
2023-02-03 00:53:19,061 epoch 7 - iter 94/942 - loss 0.06227695 - samples/sec: 53.16 - lr: 0.100000
2023-02-03 00:54:21,357 epoch 7 - iter 188/942 - loss 0.06262495 - samples/sec: 52.68 - lr: 0.100000
2023-02-03 00:55:24,840 epoch 7 - iter 282/942 - loss 0.06319853 - samples/sec: 52.03 - lr: 0.100000
2023-02-03 00:56:29,844 epoch 7 - iter 376/942 - loss 0.06314896 - samples/sec: 50.70 - lr: 0.100000
2023-02-03 00:57:32,135 epoch 7 - iter 470/942 - loss 0.06352554 - samples/sec: 52.64 - lr: 0.100000
2023-02-03 00:58:34,483 epoch 7 - iter 564/942 - loss 0.06394507 - samples/sec: 52.80 - lr: 0.100000
2023-02-03 00:59:37,860 epoch 7 - iter 658/942 - loss 0.06427541 - samples/sec: 51.91 - lr: 0.100000
2023-02-03 01:00:40,419 epoch 7 - iter 752/942 - loss 0.06488599 - samples/sec: 53.03 - lr: 0.100000
2023-02-03 01:01:45,166 epoch 7 - iter 846/942 - loss 0.06507974 - samples/sec: 50.51 - lr: 0.100000
2023-02-03 01:02:48,421 epoch 7 - iter 940/942 - loss 0.06500668 - samples/sec: 52.26 - lr: 0.100000
2023-02-03 01:02:49,507 ----------------------------------------------------------------------------------------------------
2023-02-03 01:02:49,509 EPOCH 7 done: loss 0.0651 - lr 0.100000
2023-02-03 01:02:49,510 BAD EPOCHS (no improvement): 0
2023-02-03 01:02:49,512 ----------------------------------------------------------------------------------------------------
2023-02-03 01:03:53,644 epoch 8 - iter 94/942 - loss 0.05162865 - samples/sec: 51.46 - lr: 0.100000
2023-02-03 01:04:55,827 epoch 8 - iter 188/942 - loss 0.05334153 - samples/sec: 53.39 - lr: 0.100000
2023-02-03 01:05:59,519 epoch 8 - iter 282/942 - loss 0.05392888 - samples/sec: 51.87 - lr: 0.100000
2023-02-03 01:07:04,053 epoch 8 - iter 376/942 - loss 0.05488943 - samples/sec: 50.93 - lr: 0.100000
2023-02-03 01:08:07,448 epoch 8 - iter 470/942 - loss 0.05525809 - samples/sec: 51.68 - lr: 0.100000
2023-02-03 01:09:09,007 epoch 8 - iter 564/942 - loss 0.05581386 - samples/sec: 53.79 - lr: 0.100000
2023-02-03 01:10:13,142 epoch 8 - iter 658/942 - loss 0.05637472 - samples/sec: 51.28 - lr: 0.100000
2023-02-03 01:11:16,564 epoch 8 - iter 752/942 - loss 0.05700848 - samples/sec: 52.26 - lr: 0.100000
2023-02-03 01:12:19,018 epoch 8 - iter 846/942 - loss 0.05721811 - samples/sec: 53.33 - lr: 0.100000
2023-02-03 01:13:21,662 epoch 8 - iter 940/942 - loss 0.05765041 - samples/sec: 52.50 - lr: 0.100000
2023-02-03 01:13:22,627 ----------------------------------------------------------------------------------------------------
2023-02-03 01:13:22,629 EPOCH 8 done: loss 0.0576 - lr 0.100000
2023-02-03 01:13:22,630 BAD EPOCHS (no improvement): 0
2023-02-03 01:13:22,632 ----------------------------------------------------------------------------------------------------
2023-02-03 01:14:25,549 epoch 9 - iter 94/942 - loss 0.04914415 - samples/sec: 52.90 - lr: 0.100000
2023-02-03 01:15:29,534 epoch 9 - iter 188/942 - loss 0.04840015 - samples/sec: 50.81 - lr: 0.100000
2023-02-03 01:16:32,586 epoch 9 - iter 282/942 - loss 0.04906150 - samples/sec: 52.16 - lr: 0.100000
2023-02-03 01:17:36,214 epoch 9 - iter 376/942 - loss 0.04905059 - samples/sec: 52.10 - lr: 0.100000
2023-02-03 01:18:36,501 epoch 9 - iter 470/942 - loss 0.04954600 - samples/sec: 54.30 - lr: 0.100000
2023-02-03 01:19:40,472 epoch 9 - iter 564/942 - loss 0.05023844 - samples/sec: 50.81 - lr: 0.100000
2023-02-03 01:20:43,764 epoch 9 - iter 658/942 - loss 0.05100162 - samples/sec: 52.40 - lr: 0.100000
2023-02-03 01:21:48,183 epoch 9 - iter 752/942 - loss 0.05081107 - samples/sec: 50.42 - lr: 0.100000
2023-02-03 01:22:50,500 epoch 9 - iter 846/942 - loss 0.05124117 - samples/sec: 52.76 - lr: 0.100000
2023-02-03 01:23:52,107 epoch 9 - iter 940/942 - loss 0.05123435 - samples/sec: 53.62 - lr: 0.100000
2023-02-03 01:23:53,065 ----------------------------------------------------------------------------------------------------
2023-02-03 01:23:53,067 EPOCH 9 done: loss 0.0512 - lr 0.100000
2023-02-03 01:23:53,068 BAD EPOCHS (no improvement): 0
2023-02-03 01:23:53,070 ----------------------------------------------------------------------------------------------------
2023-02-03 01:24:56,738 epoch 10 - iter 94/942 - loss 0.04079598 - samples/sec: 51.95 - lr: 0.100000
2023-02-03 01:26:00,546 epoch 10 - iter 188/942 - loss 0.04349453 - samples/sec: 51.43 - lr: 0.100000
2023-02-03 01:27:03,250 epoch 10 - iter 282/942 - loss 0.04468432 - samples/sec: 52.92 - lr: 0.100000
2023-02-03 01:28:06,822 epoch 10 - iter 376/942 - loss 0.04454834 - samples/sec: 52.09 - lr: 0.100000
2023-02-03 01:29:09,218 epoch 10 - iter 470/942 - loss 0.04464679 - samples/sec: 53.67 - lr: 0.100000
2023-02-03 01:30:12,859 epoch 10 - iter 564/942 - loss 0.04511441 - samples/sec: 51.55 - lr: 0.100000
2023-02-03 01:31:16,185 epoch 10 - iter 658/942 - loss 0.04565639 - samples/sec: 52.15 - lr: 0.100000
2023-02-03 01:32:19,331 epoch 10 - iter 752/942 - loss 0.04593389 - samples/sec: 52.21 - lr: 0.100000
2023-02-03 01:33:22,468 epoch 10 - iter 846/942 - loss 0.04593192 - samples/sec: 52.36 - lr: 0.100000
2023-02-03 01:34:24,919 epoch 10 - iter 940/942 - loss 0.04616920 - samples/sec: 53.28 - lr: 0.100000
2023-02-03 01:34:26,154 ----------------------------------------------------------------------------------------------------
2023-02-03 01:34:26,156 EPOCH 10 done: loss 0.0462 - lr 0.100000
2023-02-03 01:34:26,158 BAD EPOCHS (no improvement): 0
2023-02-03 01:34:27,485 ----------------------------------------------------------------------------------------------------
2023-02-03 01:34:27,488 Testing using last state of model ...
2023-02-03 01:34:45,542 Evaluating as a multi-label problem: False
2023-02-03 01:34:45,828 0.9666	0.9666	0.9666	0.9666
2023-02-03 01:34:45,829 
Results:
- F-score (micro) 0.9666
- F-score (macro) 0.9409
- Accuracy 0.9666

By class:
              precision    recall  f1-score   support

        NOUN     0.9438    0.9587    0.9512      6373
       PUNCT     0.9977    0.9959    0.9968      4367
         ADP     0.9827    0.9860    0.9843      3917
        VERB     0.9764    0.9731    0.9747      3786
         DET     0.9920    0.9953    0.9936      3599
       PROPN     0.9444    0.8665    0.9038      2802
        PRON     0.9811    0.9912    0.9861      2613
         ADJ     0.9210    0.9629    0.9415      2456
         AUX     0.9751    0.9909    0.9829      1860
         ADV     0.9631    0.9409    0.9518      1607
       CCONJ     0.9884    0.9728    0.9805      1139
         NUM     0.9497    0.9805    0.9649       771
       SCONJ     0.9362    0.9375    0.9368       720
        PART     0.9893    0.9938    0.9916       650
           X     0.7847    0.6243    0.6954       181
         SYM     0.8092    0.9318    0.8662       132
        INTJ     0.8926    0.8926    0.8926       121

    accuracy                         0.9666     37094
   macro avg     0.9428    0.9409    0.9409     37094
weighted avg     0.9665    0.9666    0.9663     37094

2023-02-03 01:34:45,831 ----------------------------------------------------------------------------------------------------
2023-02-03 01:34:45,833 ----------------------------------------------------------------------------------------------------
2023-02-03 01:34:58,501 Evaluating as a multi-label problem: False
2023-02-03 01:34:58,684 /root/.flair/datasets/ud_english
2023-02-03 01:34:58,686 0.9645	0.9645	0.9645	0.9645
2023-02-03 01:34:58,687 ----------------------------------------------------------------------------------------------------
2023-02-03 01:35:04,006 Evaluating as a multi-label problem: False
2023-02-03 01:35:04,100 /root/.flair/datasets/ud_spanish
2023-02-03 01:35:04,102 0.9709	0.9709	0.9709	0.9709
